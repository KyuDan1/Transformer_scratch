{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJlE3Mgh78iJ"
      },
      "outputs": [],
      "source": [
        "# %pip install bertviz datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxJ0xwvLHhj_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yea5Gi0WK6JH"
      },
      "outputs": [],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nxF9CFz7Mbc"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from bertviz.transformers_neuron_view import BertModel\n",
        "from bertviz.neuron_view import show\n",
        "\n",
        "model_ckpt = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model = BertModel.from_pretrained(model_ckpt)\n",
        "text = \"time flies like an arrow\"\n",
        "#show(model, \"bert\", tokenizer, text, display_mode = \"light\", layer=0, head=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KskH77KO7-fl"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(text, return_tensors = \"pt\", add_special_tokens=False)\n",
        "inputs.input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Gxp8yNQ9skt"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4H1bHwp87KA"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from transformers import AutoConfig\n",
        "config = AutoConfig.from_pretrained(model_ckpt)\n",
        "token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "token_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "684DsIYIVJlF"
      },
      "outputs": [],
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQH3j3byVMDb"
      },
      "outputs": [],
      "source": [
        "config.intermediate_size = 768*2\n",
        "config.num_attention_heads = 2\n",
        "config.num_hidden_layers = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy5cObQ0VjXP"
      },
      "outputs": [],
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90HY0up-9344"
      },
      "outputs": [],
      "source": [
        "inputs_embeds = token_emb(inputs.input_ids)\n",
        "inputs_embeds.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVXNAC1Y-0sc"
      },
      "outputs": [],
      "source": [
        "inputs_embeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooh6ZniU-2GQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from math import sqrt\n",
        "query = key = value = inputs_embeds\n",
        "dim_k = sqrt(key.size(-1))\n",
        "scores = torch.bmm(query, key.transpose(1, 2)) / dim_k\n",
        "scores.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxhuv7EU_yTg"
      },
      "outputs": [],
      "source": [
        "scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8ZwD9jT_0NM"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "weights = F.softmax(scores, dim=-1)\n",
        "weights.sum(dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUKOIqzCCvto"
      },
      "outputs": [],
      "source": [
        "weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9dNc-5xCx99"
      },
      "outputs": [],
      "source": [
        "attn_outputs = torch.bmm(weights, value)\n",
        "attn_outputs.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h861PH_mDZ66"
      },
      "outputs": [],
      "source": [
        "attn_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myD3CLpcDz-O"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(query, key, value):\n",
        "  dim_k = query.size(-1)\n",
        "  scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n",
        "  weights = F.softmax(scores, dim=-1)\n",
        "  return torch.bmm(weights, value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psvQzvfhD8_Z"
      },
      "outputs": [],
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, embed_dim, head_dim):\n",
        "    super().__init__()\n",
        "    self.q = nn.Linear(embed_dim, head_dim)\n",
        "    self.k = nn.Linear(embed_dim, head_dim)\n",
        "    self.v = nn.Linear(embed_dim, head_dim)\n",
        "\n",
        "  def forward(self, hidden_state):\n",
        "    attn_outputs = scaled_dot_product_attention(self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))\n",
        "    return attn_outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2W9tBJl9FtJ0"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    embed_dim = config.hidden_size\n",
        "    num_heads = config.num_attention_heads\n",
        "    head_dim = embed_dim // num_heads\n",
        "    self.heads = nn.ModuleList(\n",
        "        [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n",
        "    )\n",
        "    self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "  def forward(self, hidden_states):\n",
        "    x = torch.cat([h(hidden_states) for h in self.heads], dim=-1)\n",
        "    x = self.output_linear(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F63T8168L9vE"
      },
      "outputs": [],
      "source": [
        "multihead_attn = MultiHeadAttention(config)\n",
        "#multihead_attn = multihead_attn.to(device)\n",
        "#inputs_embeds = inputs_embeds.to(device)\n",
        "\n",
        "attn_output = multihead_attn(inputs_embeds)\n",
        "attn_output.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFeAQ9AmRCe7"
      },
      "outputs": [],
      "source": [
        "from bertviz import head_view\n",
        "from transformers import AutoModel\n",
        "\n",
        "model = AutoModel.from_pretrained(model_ckpt, output_attentions=True)\n",
        "\n",
        "sentence_a = \"time flies like an arrow\"\n",
        "sentence_b = \"fruit flies like a banana\"\n",
        "viz_inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt')\n",
        "viz_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kYdb_gIReml"
      },
      "outputs": [],
      "source": [
        "attention = model(**viz_inputs).attentions\n",
        "sentence_b_start = (viz_inputs.token_type_ids ==0).sum(dim=1)\n",
        "tokens = tokenizer.convert_ids_to_tokens(viz_inputs.input_ids[0])\n",
        "head_view(attention, tokens, sentence_b_start, heads=[8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0G_M_tSRu9s"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "    self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "    self.gelu = nn.GELU()\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear_1(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.linear_2(x)\n",
        "    x = self.dropout(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wjBfagZVd29"
      },
      "outputs": [],
      "source": [
        "feed_forward = FeedForward(config)\n",
        "ff_outputs = feed_forward(attn_outputs)\n",
        "ff_outputs.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0wsvoTGVhxt"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
        "    self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
        "    self.attention = MultiHeadAttention(config)\n",
        "    self.feed_forward = FeedForward(config)\n",
        "\n",
        "  def forward(self, x):\n",
        "    hidden_state = self.layer_norm_1(x)\n",
        "    x = x + self.attention(hidden_state)\n",
        "    x = x + self.feed_forward(self.layer_norm_2(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rm9GLk5IMTJj"
      },
      "outputs": [],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNI2HpjAdFRo"
      },
      "outputs": [],
      "source": [
        "encoder_layer = TransformerEncoderLayer(config)\n",
        "#encoder_layer = encoder_layer.to(device)\n",
        "#inputs_embeds = inputs_embeds.to(device)\n",
        "\n",
        "x = encoder_layer(inputs_embeds)\n",
        "x.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRuwzVkndIqq"
      },
      "outputs": [],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKSoGjAadK2x"
      },
      "outputs": [],
      "source": [
        "class Embeddings(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.token_embddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "    self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "    self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
        "    self.dropout = nn.Dropout()\n",
        "\n",
        "  def forward(self, input_ids):\n",
        "    seq_length = input_ids.size(1)\n",
        "    position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device).unsqueeze(0)\n",
        "    token_embeddings = self.token_embddings(input_ids)\n",
        "    position_embeddings = self.position_embeddings(position_ids)\n",
        "    embeddings = token_embeddings + position_embeddings\n",
        "    embeddings = self.layer_norm(embeddings)\n",
        "    embeddings = self.dropout(embeddings)\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oefWLuvOk2l1"
      },
      "outputs": [],
      "source": [
        "embedding_layer = Embeddings(config)\n",
        "embedding_layer(inputs.input_ids).size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQF217TtlG-f"
      },
      "outputs": [],
      "source": [
        "embedding_layer(inputs.input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuS5aAVLlHUf"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.embeddings = Embeddings(config)\n",
        "    self.layers = nn.ModuleList([TransformerEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embeddings(x)\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKMBPkhymmqi"
      },
      "outputs": [],
      "source": [
        "encoder = TransformerEncoder(config)\n",
        "encoder(inputs.input_ids).size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdQjz_O6m7Y_"
      },
      "outputs": [],
      "source": [
        "seq_len = inputs.input_ids.size(1)\n",
        "mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\n",
        "mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJ03m_aZpNGc"
      },
      "outputs": [],
      "source": [
        "scores.masked_fill(mask == 0, -float(\"inf\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mFOo1OD9d-x"
      },
      "source": [
        "## Masked Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGnHxbISplQ3"
      },
      "outputs": [],
      "source": [
        "def masked_scaled_dot_product_attention(query, key, value, mask=None):\n",
        "  dim_k = query.size(-1)\n",
        "  scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n",
        "  if mask is not None:\n",
        "    scores = scores.masked_fill(mask == 0, -float(\"inf\"))\n",
        "  weights = F.softmax(scores, dim=-1)\n",
        "  return torch.bmm(weights, value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LWOKV3utjwC"
      },
      "outputs": [],
      "source": [
        "class MaskedAttentionHead(nn.Module):\n",
        "  def __init__(self, embed_dim, head_dim):\n",
        "    super().__init__()\n",
        "    self.q = nn.Linear(embed_dim, head_dim)\n",
        "    self.k = nn.Linear(embed_dim, head_dim)\n",
        "    self.v = nn.Linear(embed_dim, head_dim)\n",
        "\n",
        "  def forward(self, hidden_state):\n",
        "    attn_outputs = masked_scaled_dot_product_attention(self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))\n",
        "    return attn_outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4Rlup8ytY6d"
      },
      "outputs": [],
      "source": [
        "class MaskedMultiHeadAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    embed_dim = config.hidden_size\n",
        "    num_heads = config.num_attention_heads\n",
        "    head_dim = embed_dim // num_heads\n",
        "    self.heads = nn.ModuleList(\n",
        "        [MaskedAttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n",
        "    )\n",
        "    self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "  def forward(self, hidden_states):\n",
        "    x = torch.cat([h(hidden_states) for h in self.heads], dim=-1)\n",
        "    x = self.output_linear(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y033cZyG9aKR"
      },
      "source": [
        "## Encoder-Decoder Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRRtcAmmuPIF"
      },
      "outputs": [],
      "source": [
        "class EncDecAttentionHead(nn.Module):\n",
        "  def __init__(self, embed_dim, head_dim):\n",
        "    super().__init__()\n",
        "    self.q = nn.Linear(embed_dim, head_dim)\n",
        "    self.k = nn.Linear(embed_dim, head_dim)\n",
        "    self.v = nn.Linear(embed_dim, head_dim)\n",
        "\n",
        "  def forward(self, enc_hidden_state, dec_hidden_state):\n",
        "    attn_outputs = scaled_dot_product_attention(self.q(dec_hidden_state), self.k(enc_hidden_state), self.v(enc_hidden_state))\n",
        "    return attn_outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7WEOPpfz3vM"
      },
      "outputs": [],
      "source": [
        "class EncDecMultiHeadAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    embed_dim = config.hidden_size\n",
        "    num_heads = config.num_attention_heads\n",
        "    head_dim = embed_dim // num_heads\n",
        "    self.heads = nn.ModuleList(\n",
        "        [EncDecAttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n",
        "    )\n",
        "    self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "  def forward(self, enc_hidden_state, dec_hidden_state):\n",
        "    x = torch.cat([h(enc_hidden_state, dec_hidden_state) for h in self.heads], dim=-1)\n",
        "    x = self.output_linear(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_LWVz-QqccV"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
        "    self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
        "    self.layer_norm_3 = nn.LayerNorm(config.hidden_size)\n",
        "    self.masked_attention = MaskedMultiHeadAttention(config)\n",
        "    self.encdecattention = EncDecMultiHeadAttention(config)\n",
        "    self.feed_forward = FeedForward(config)\n",
        "\n",
        "  def forward(self, x, enc_hidden_state):\n",
        "    hidden_state = self.layer_norm_1(x)\n",
        "    x = x + self.masked_attention(hidden_state)\n",
        "    x = self.layer_norm_2(x)\n",
        "    x = x + self.encdecattention(enc_hidden_state, x)\n",
        "    x = self.feed_forward(self.layer_norm_3(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZt5SjagvT-0"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.embeddings = Embeddings(config)\n",
        "    self.layers = nn.ModuleList([TransformerDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "  def forward(self, x, enc_hidden_state):\n",
        "    x = self.embeddings(x)\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, enc_hidden_state)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyCtSbnIvof9"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.encoder = TransformerEncoder(config)\n",
        "    self.decoder = TransformerDecoder(config)\n",
        "    self.output = nn.Linear(config.hidden_size, config.vocab_size)\n",
        "    #self.softmax = nn.Softmax(dim=-1) softmax는 transformer 안에 있으면 안된다고 함.\n",
        "\n",
        "\n",
        "    # 가중치 초기화\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "        if module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "        module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "    elif isinstance(module, nn.LayerNorm):\n",
        "        module.bias.data.zero_()\n",
        "        module.weight.data.fill_(1.0)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, enc_input, dec_input):\n",
        "    enc_hidden_state = self.encoder(enc_input)\n",
        "    dec_hidden_state = self.decoder(dec_input, enc_hidden_state)\n",
        "    output = self.output(dec_hidden_state)\n",
        "    #output = self.softmax(output)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coLNufd1wH54"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhM8XAr906Q1"
      },
      "outputs": [],
      "source": [
        "transformer(inputs.input_ids, inputs.input_ids).size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsKrTNOh6R-r"
      },
      "outputs": [],
      "source": [
        "prob = transformer(inputs.input_ids, inputs.input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c83s-y166XYp"
      },
      "outputs": [],
      "source": [
        "prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrp69REh15Nf"
      },
      "outputs": [],
      "source": [
        "text = \"time flies like an arrow\"\n",
        "text2 = \"시간이 참 빨리 간다\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXTOsTFM1zDx"
      },
      "outputs": [],
      "source": [
        "text = \"time flies like an arrow\"\n",
        "inputs2 = tokenizer(text2, return_tensors = \"pt\", add_special_tokens=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSrWB6_G2Agh"
      },
      "outputs": [],
      "source": [
        "inputs2.input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHEu4tA12FJ4"
      },
      "outputs": [],
      "source": [
        "transformer(inputs.input_ids, inputs2.input_ids).size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apkw-TZhKwoG"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APGk5OmJK6K6"
      },
      "outputs": [],
      "source": [
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veSs_OL5KwE7"
      },
      "outputs": [],
      "source": [
        "class TranslationTrainer:\n",
        "    def __init__(self, model, optimizer, criterion):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        pad_token_id = 0\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
        "\n",
        "    def train_step(self, enc_input, dec_input, target):\n",
        "        \"\"\"\n",
        "        enc_input: 영어 문장의 토큰 시퀀스 [batch_size, enc_seq_len]\n",
        "        dec_input: 한글 문장의 토큰 시퀀스 (시작 토큰 포함, 마지막 토큰 제외)\n",
        "                  [batch_size, dec_seq_len]\n",
        "        target: 한글 문장의 토큰 시퀀스 (시작 토큰 제외)\n",
        "                [batch_size, dec_seq_len]\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        output = self.model(enc_input, dec_input)\n",
        "\n",
        "        # output = torch.Size([batch, decoder_input_length, vocab_size])\n",
        "\n",
        "        output = output.contiguous().view(-1, output.size(-1))  # [batch_size * dec_seq_len, vocab_size]\n",
        "        target = target.contiguous().view(-1)  # [batch_size * dec_seq_len]\n",
        "\n",
        "        # loss 계산\n",
        "        loss = self.criterion(output, target)\n",
        "\n",
        "        # 역전파\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEcPdTBKNdGx"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"msarmi9/korean-english-multitarget-ted-talks-task\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WHGL0F-Nnfs"
      },
      "outputs": [],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln7wzyqINy_G"
      },
      "outputs": [],
      "source": [
        "ds[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6cBnbKsOGDe"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, hf_dataset, eng_tokenizer, kor_tokenizer, max_length=128):\n",
        "        \"\"\"\n",
        "        hf_dataset: HuggingFace dataset (train/validation/test split)\n",
        "        eng_tokenizer: 영어 토크나이저\n",
        "        kor_tokenizer: 한글 토크나이저\n",
        "        max_length: 최대 시퀀스 길이\n",
        "        \"\"\"\n",
        "        self.dataset = hf_dataset\n",
        "        self.eng_tokenizer = eng_tokenizer\n",
        "        self.kor_tokenizer = kor_tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # HF dataset에서 데이터 가져오기\n",
        "        item = self.dataset[idx]\n",
        "        eng_sent = item['english']\n",
        "        kor_sent = item['korean']\n",
        "\n",
        "        # 토큰화\n",
        "\n",
        "        eng_tokens = self.eng_tokenizer(eng_sent,\n",
        "                                        max_length = self.max_length,\n",
        "                                        truncation=True,\n",
        "                                        padding='max_length',\n",
        "                                        return_tensors = \"pt\")\n",
        "        kor_tokens = self.kor_tokenizer(kor_sent,\n",
        "                                        max_length = self.max_length,\n",
        "                                        truncation=True,\n",
        "                                        padding='max_length',\n",
        "                                        return_tensors = \"pt\")\n",
        "        return {\n",
        "            'eng_tokens': eng_tokens['input_ids'].squeeze(0),  # [seq_len]\n",
        "            'eng_attention_mask': eng_tokens['attention_mask'].squeeze(0),  # [seq_len]\n",
        "            'kor_tokens': kor_tokens['input_ids'].squeeze(0),  # [seq_len]\n",
        "            'kor_attention_mask': kor_tokens['attention_mask'].squeeze(0)  # [seq_len]\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKv4eUMMPfA7"
      },
      "outputs": [],
      "source": [
        "model_ckpt = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "eng_tokenizer = tokenizer\n",
        "kor_tokenizer = tokenizer\n",
        "datasetloader = TranslationDataset(ds, tokenizer, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEnuKxXGQ7CW"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    BERT 토크나이저는 이미 패딩을 처리하므로 단순히 배치로 묶어주기만 하면 됩니다\n",
        "    \"\"\"\n",
        "    return {\n",
        "        'eng_tokens': torch.stack([item['eng_tokens'] for item in batch]),\n",
        "        'eng_attention_mask': torch.stack([item['eng_attention_mask'] for item in batch]),\n",
        "        'kor_tokens': torch.stack([item['kor_tokens'] for item in batch]),\n",
        "        'kor_attention_mask': torch.stack([item['kor_attention_mask'] for item in batch])\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdG4RimWQ_j5"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def create_dataloaders(dataset_dict, eng_tokenizer, kor_tokenizer,\n",
        "                      batch_size=32, max_length=64):\n",
        "    dataloaders = {}\n",
        "\n",
        "    for split in ['train', 'validation', 'test']:\n",
        "        dataset = TranslationDataset(\n",
        "            dataset_dict[split],\n",
        "            eng_tokenizer=eng_tokenizer,\n",
        "            kor_tokenizer=kor_tokenizer,\n",
        "            max_length=max_length\n",
        "        )\n",
        "\n",
        "        dataloaders[split] = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=(split == 'train'),\n",
        "            collate_fn=collate_fn,\n",
        "            num_workers=4\n",
        "        )\n",
        "\n",
        "    return dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tseOBrOrPLge"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "config.intermediate_size = 768*2\n",
        "config.num_attention_heads = 4\n",
        "config.num_hidden_layers = 6\n",
        "config.hidden_dropout_prob = 0.2  # 기존보다 높게 설정\n",
        "config.attention_probs_dropout_prob = 0.2\n",
        "\n",
        "\n",
        "model = Transformer(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-dtMgCsQaz9"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CK09SBYNR6DE"
      },
      "outputs": [],
      "source": [
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "trainer = TranslationTrainer(model, optimizer, nn.CrossEntropyLoss(label_smoothing=0.1))\n",
        "dataloader = create_dataloaders(ds, eng_tokenizer, kor_tokenizer, batch_size=batch_size)\n",
        "val_dataloader = dataloader['validation']\n",
        "train_dataloader = dataloader['train']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHs2XFu0fYSh"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "\"\"\"scheduler = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=1e-3,\n",
        "    steps_per_epoch=len(train_dataloader),\n",
        "    epochs=num_epochs\n",
        ")\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxV4ycE2fYSh"
      },
      "outputs": [],
      "source": [
        "def validate(model, val_dataloader, tokenizer, num_examples=5):\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, label_smoothing=0.1)\n",
        "\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "\n",
        "    examples = []\n",
        "    example_count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            enc_input = batch['eng_tokens'].to(device)\n",
        "            dec_input = batch['kor_tokens'][:, :-1].to(device)\n",
        "            target = batch['kor_tokens'][:, 1:].to(device)\n",
        "\n",
        "            output = model(enc_input, dec_input)\n",
        "            loss = criterion(output.view(-1, output.size(-1)), target.view(-1))\n",
        "            total_val_loss += loss.item()\n",
        "        if example_count < num_examples:\n",
        "            pred_tokens = output[0].argmax(dim=-1)\n",
        "            src_text = tokenizer.decode(enc_input[0], skip_special_tokens = True)\n",
        "            target_full = batch['kor_tokens'][0].to(device)\n",
        "            tgt_text = tokenizer.decode(target_full, skip_special_tokens = True)\n",
        "\n",
        "            pred_text = tokenizer.decode(pred_tokens, skip_special_tokens=True)\n",
        "\n",
        "            examples.append({\n",
        "                'source': src_text,\n",
        "                'target' : tgt_text,\n",
        "                'prediction' : pred_text\n",
        "            })\n",
        "            for i, example in enumerate(examples, 1):\n",
        "                print(f\"\\nExample {i}:\")\n",
        "                print(f\"Source    : {example['source']}\")\n",
        "                print(f\"Target    : {example['target']}\")\n",
        "                print(f\"Prediction: {example['prediction']}\")\n",
        "                print(\"-\" * 50)\n",
        "            example_count +=1\n",
        "\n",
        "    return total_val_loss / len(val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llxLm35fPfXi",
        "outputId": "5f66ec91-8506-4cf1-8d1a-5ebedcd6bdbe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/25980 [00:33<?, ?it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅳᅡᅡᅡᅡᅡᅡᅡᅳᅡᅡᅳᅡᅡᅡᅳᅡᅡᅳᅡᅡᅡᅳᅡᅳᅳᅳᅡᅡᅳᅡᅡᅳᅡᅡᅡᅳᅳᅳᅳᅳᅳᅡᅳᅡᅳᅡᅡᅳᅡᅡᅳᅳᅳᅡᅳᅳᅳᅡᅡᅳᅳᅡ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫᆫ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡᅡ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᆫᅵᆫᅵᆫ인ᅵᆫ인ᆫᅡᅵᆫ인ᆫᅵᆫᅵᆫ인인ᅵᆫᆫᅵᆫ인인ᄋᆫ인ᅵᆫᅵᆫ인ᄋᆫᅵᆫᅵᆫ인\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᆫᅡᆫᅡᆫᆫᅡᆫᅡᆫᆫᅡᆫᆫᆫᅡᆫᆫᅡᆫᆫᅡᆫᅡᆫᆫᅡᆫᆫᅡᆫᅡᆫᆫᅡᆫᆫᅡᆫᆫᅡᆫᆫᆫᆫᅡᆫᅡᆫᅡᆫᆫᅡᆫᆫᆫᅡᆫᅡᆫᆫᅡᆫ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##아아ᄋ아안아ᄋᄋ아ᄋ아ᄋ아아ᄋ아ᄋ아안안아ᄋ아ᄋᄋᄋ아아아ᄋ안ᄋ안ᅡᄋ안\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᆫᅵᆫᅵᆫᆫᅵᆫᅵᆫᆫᅵᆫᆫᆫᅵᆫᆫᅵᆫᆫᅵᆫᅵᆫᆫᅵᆫᆫᅵᆫᅵᆫᆫᅡᆫᆫᅵᆫᆫᅵᆫᆫᆫᆫᅵᆫᅵᆫᅡᆫᆫᅵᆫᆫᆫᅵᆫᅵᆫᆫᅵᆫ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᆫᅡᆫᅡᆫᆫᅡᆫᅡᆫᆫᅡᆫᆫᆫᅡᆫᆫᅡᆫᆫᅡᆫᅡᆫᆫᅡᆫᆫᅡᆫᅡᆫᆫᅡᆫᆫᅡᆫᆫᅡᆫᆫᆫᆫᅡᆫᅡᆫᅡᆫᆫᅡᆫᆫᆫᅡᆫᅡᆫᆫᅡᆫ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##아아ᄋ아안아ᄋᄋ아ᄋ아ᄋ아아ᄋ아ᄋ아아ᄋ아ᄋ아ᄋ아ᄋᄋᄋ아아아ᄋ안ᄋ아아ᄋ아ᄋ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᆫᅡᆫᅡᆫᆫᅡᆫᅡᆫᆫᅡᆫᆫᆫᅡᆫᆫᅡᆫᆫᅡᆫᅡᆫᆫᅡᆫᆫᅡᆫᅡᆫᆫᅡᆫᆫᅡᆫᆫᅡᆫᆫᆫᆫᅡᆫᅡᆫᅡᆫᆫᅡᆫᆫᆫᅡᆫᅡᆫᆫᅡᆫ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᆫᅡᆫᅡᆫᆫᅡᆫᅡᆫᆫᅡᆫᆫᆫᅡᆫᆫᅡᆫᆫᅡᆫᅡᆫᆫᅡᆫᆫᅡᆫᅡᆫᆫᅡᆫᆫᅡᆫᆫᅡᆫᆫᆫᆫᅡᆫᅡᆫᅡᆫᆫᅡᆫᆫᆫᅡᆫᅡᆫᆫᅡᆫ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᆫᅡᆫᅡᆫᆫᅡᆫᅡᆫᆫᅡᆫᆫᆫᅡᆫᆫᅡᆫᆫᅡᆫᅡᆫᆫᅡᆫᆫᅡᆫᅡᆫᆫᅡᆫᆫᅡᆫᆫᅡᆫᆫᆫᆫᅡᆫᅡᆫᅡᆫᆫᅡᆫᆫᆫᅡᆫᅡᆫᆫᅡᆫ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᆫᅵᆫᅵᆫᆫᅵᆫᅵᆫᆫᅵᆫᆫᆫᅵᆫᆫᅵᆫᆫᅵᆫᅵᆫᆫᅵᆫᆫᅵᆫᅵᆫᆫᅵᆫᆫᅵᆫᆫᅵᆫᆫᆫᆫᅵᆫᅵᆫᅵᆫᆫᅵᆫᆫᆫᅵᆫᅵᆫᆫᅵᆫ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᆫᅡ아ᄋ안ᅡᆫ아ᄋᄋ안아ᄋ안ᅡᆫ아ᄋ안ᅡᆫ안안안ᄋᄋ안ᅡᆫᅡᄋ안ᄋ안ᅡᆫ안\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᆫᅡᆫᅡᆫᆫᅡᆫᅡᆫᆫᅡᆫᆫᆫᅡᆫᆫᅡᆫᆫᅡᆫᅡᆫᆫᅡᆫᆫᅡᆫᅡᆫᆫᅡᆫᆫᅡᆫᆫᅡᆫᆫᆫᆫᅡᆫᅡᆫᅡᆫᆫᅡᆫᆫᆫᅡᆫᅡᆫᆫᅡᆫ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##아아ᄋ안ᅡᆫ아ᄋᄋ안아ᄋ안ᅡᄋ아ᄋ안ᅡᆫ안안안ᄋᄋ아안ᅡᄋ안ᄋ안ᅡᆫ안\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##아아안ᅡᆫ아ᄋ안아안ᅡᆫᅡᄋ안ᅡᆫ안안ᅡᆫ안ᅡᆫᅡ안안ᅡᆫ안\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡ안ᅡᆫᅡᆫᅡ안ᅡ안ᅡᆫᅡ안ᅡᆫᅡᆫᅡᆫᅡᆫᅡᆫᅡᆫᅡ안ᅡᆫᅡᆫᅡᆫ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##아아ᄅ안ᅡᆫ아ᄅᄋ안아ᄋ안ᅡᆫ아ᄋ안ᅡᆫ안안안ᄋᄋ안ᅡᆫᅡᄋ안ᄋ안ᅡᆫ안\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡ안ᅡᆫᅡᆫᅡᆫᅡᆫ아안ᅡᆫᅡᆫ안ᅡᆫᅡᆫᅡᆫᅡᆫᅡᆫᅡᆫᅡᆫᅡᆫᅡᆫᅡᆫ안\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 안ᅡᆫ안ᅡᆫ안ᄋ안안안ᅡᆫ안안ᅡᆫ안안안ᄋᄋ안ᅡᆫᅡᆫ안ᄋ안ᅡᆫ안\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡ안안ᅡᆯ안ᅡᆫ안ᅡᆫᅡᆫ안안ᅡᆫᅡᆫ안안ᅡᆫᅡᆫᅡᆫᅡᆯᅡᆫᅡᆫ안\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅵ인인ᅵᆫ인ᅵᆫ인ᅵᆫᅵᆫ이ᄋ인ᅵᆫᅵᆫ인안인ᅡᆫᅵᄋ인인ᅵᆫ인\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅡᆫᅡᄋ안ᅡᆯ안ᅡᆫ안ᅡᆫᅡᆫ안안ᅡᆫᅡᆫ안안안ᅡᆫᅡᄋ알안ᅡᆫ아ᄋ\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅳ으ᄋ인ᅵᆫ은ᅡᆫ이안ᅵᆫ은은ᅡᆫᅡᆫ은안ᅡᆫᅡᆫᅵᄋ은안ᅵᆫ이\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅳᆫᅳᆫ인ᅵᆫ은ᅡᆫ인ᅡᆫᅵᆫ은은ᅡᆫᅡᆫ은안인ᅡᆫᅵᆫ은안ᅵᆫ이\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅳᆫᅳᆫ인ᅡᆫ은ᅡᆫ안ᅡᆫᅡᆫ은은ᅡᆫᅡᆫ은안안ᅡᆫᅡᆫ은안ᅡᆫ아\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅳᆫᅵᆫ안ᅵᆫ은ᅡᆫ인ᅡᆫᅡᆫ은은ᅡᆫᅡᆫ은안안ᅡᆫᅡᆫᅳᆫ안ᅡᆫ아\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅳᆫᅵᆫ인ᅳᆯ은ᅡᆫ은ᅡᆫᅵᆫ은은ᅳᆫᅡᆫ은안ᅡᆫᅡᆫᅡᆫᅳᆯ안ᅵᆫ으\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 은ᅵᆫ인ᅳᆫ은 안은ᅡᆫᅵᆫ은은ᅡᆫᅡᆫ은안ᄋ ᄋ 인ᅡᆫᅵᆫ은안ᅵᆫ으\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 은ᅵᆫ인ᅳᆫ은 안응ᅡᆫᅵᆫ은은ᅡᆫᅡᆫ은ᅡᆫᄋ 인ᅡᆫᅵᆫᅳᆫᅡᆫᅵᆫ으\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 근ᅵᆫ안ᅳᆫ은ᅡᆫ은ᅡᆫᅵᆫ은은ᅳᆫᅡᆫ은안ᄋ 긴ᅡᆫᅵᆫ은안ᅵᆫ으\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 은ᅵᆫ안ᅳᆫ인ᅡᆫ은ᅡᆫᅵᆫ인은ᅡᆫᅡᆫ인ᅡᆫᄋ 인ᅡᆫᅵᆫᅵᆫᅡᆫᅵᆫ아\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 근ᅵᆫ안ᅳᆫ인ᅡᆫ응ᅡᆫᅵᆫᅵᆫ은ᅡᆫᅡᆫ인ᅡᆫᅵᆫᅡᆫᅵᆫᅵᆯᅡᆫᅵᆫ아\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 은ᅵᆫ안ᅳᆯ일ᅡᆫ응ᅡᆫᅵᆫ은을ᅡᆫᅡᆫ인안ᄋ 일ᅡᆫᅵᆯ일안ᅵᆫ아\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 은ᅵᆫ안ᅡᆫ인ᅡᆫ앙ᅡᆫᅵᆫᅵᆫ은ᅡᆫᅡᆫ인ᅡᆫᅵᆫᅡᆫᅵᆫᅵᆫᅡᆫᅵᆫ아\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 근ᅵᆫ안ᅡᆫ인 안앙ᅡᆫᅵᆫ인는ᅡᆫᅡᆫ인안ᄋ 인ᅡᆫᅵᆯ인안ᅡᆫ아\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 으리ᄃ인ᅡᆫ인 안앙ᅡᆫᅵᆫ인닌ᅡᆫᅡᆫ인안ᄋ 인ᅡᆫᅵᆫ일안ᅵᆫ아\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅳ린인ᅳᆫ인ᅡᆫ응ᅡᆫᅵᆫ인닌ᅡᆫᅡᆫ인안ᄋ 인ᅡᆼᅵᆫᅵᆯᅡᆫᅵᆫ아\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅳ리ᄂ인ᅡᆯ일ᅡᆼ앙ᅡᆫᅵᆯ일닐ᅡᆫᅡᆫ인안ᄋ. 일ᅡᆼᅵᆯᅵᆯ.ᅡᆫᅵᆫ아\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 으리ᄃ안ᅡᆫ인 안앙ᅡᆫᅵᆫ인닌ᅡᆫᅡᆫ인안ᄋ 인ᅡᆫᅵᆫ일안ᅡᆫ아\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: ##ᅳ리ᄃ인ᅡᆫ인ᅡᆫ앙ᅡᆫᅵᆫ인인ᅡᆫᅡᆫ인안인ᅡᆫᅵᆫᅵᆯᅡᆫᅵᆫ아\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 아리ᄃ안ᅡᆫ이ᄃ 안앙ᅡᆫᅵᆫ이ᄃ닌ᅡᆫ 안인안ᄋ 인ᅡᆫᅵᆫᅵᆯ.ᅡᆫᅡᆫ아\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 으린안ᅳᆯ이ᄃ 안앙ᅡᆫᅵᆫ이ᄃ일ᅡᆫᅡᆫ인안ᄋ 일ᅡᆫᅵᆯᅵᆯᅡᆫᅵᆫ아\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 가린안ᅳᆯ이ᄃ 안앙ᅡᆫᅵᆫ인인ᅡᆫᅡᆫ인안인ᅡᆫᅵᆫ인안ᅵᆫ아\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 가릴아을이ᅡᆫ앙 사인이ᄃ이단ᅡᆫ잉안이당ᅵᆫᅵᆫᅡᆫᅡᆫ아\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 가릴아을이 안아ᄋ 사인이ᄃ이단 안인안이다닌ᅵᆫᅡᆸᅡ.아\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 가릴아을이 ᄉ 안아 사진이ᄃ이담 안읜안 ᄉ 이다딘ᅳᆫᅡᆫᅡᆫ아\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 고릴라을이, 악오 사진인니다. 앙연한. 이다딘은 삽ᅡᆸ아\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 고릴라들이, 악대 사진입니다. 강연한, 이 사진은 사람도\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 고릴라들이, 악대 사진입니다. 상연한, 이 사진은 사람드\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Source    : these gorillas were murdered, some would even say crucified, and unsurprisingly, they sparked international outrage.\n",
            "Target    : 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "Prediction: 고릴라들이, 학대 사진입니다. 당연한, 이 사진은 사람드\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[68], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m dec_input \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkor_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m][:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m target \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkor_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m][:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 16\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Gradient Clipping\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Update learning rate\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#scheduler.step()\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "Cell \u001b[0;32mIn[54], line 32\u001b[0m, in \u001b[0;36mTranslationTrainer.train_step\u001b[0;34m(self, enc_input, dec_input, target)\u001b[0m\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(output, target)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# 역전파\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[0;32m~/anaconda3/envs/py3.12/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/py3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import wandb\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "progress_bar = tqdm(total=total_steps, desc=\"Training\")\n",
        "run = wandb.init(project=\"transformer\")\n",
        "\n",
        "run.watch(model)\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, batch in enumerate(train_dataloader):\n",
        "        enc_input = batch['eng_tokens'].to(device)\n",
        "        dec_input = batch['kor_tokens'][:, :-1].to(device)\n",
        "        target = batch['kor_tokens'][:, 1:].to(device)\n",
        "\n",
        "        loss = trainer.train_step(enc_input, dec_input, target)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "\n",
        "        # Update learning rate\n",
        "        #scheduler.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            val_loss = validate(model, val_dataloader=val_dataloader, tokenizer=tokenizer)\n",
        "            run.log({\"val_loss\": val_loss})\n",
        "        if batch_idx:\n",
        "            run.log({\"loss\": loss})\n",
        "\n",
        "        total_loss += loss\n",
        "        progress_bar.update(1)\n",
        "        progress_bar.set_postfix({\"loss\": f\"{loss:.4f}\"})\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reJ0nvEWfYSh"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'test_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0y9li3EwfYSh"
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0M-Fnu6jfYSh"
      },
      "outputs": [],
      "source": [
        "text = \"time flies like an arrow\"\n",
        "inputs2 = tokenizer(text2, return_tensors = \"pt\", add_special_tokens=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9Q94jFmfYSi"
      },
      "outputs": [],
      "source": [
        "decoder_text = \"시간이\"\n",
        "inputs3 = tokenizer(decoder_text, return_tensors = \"pt\", add_special_tokens=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PO_NK3ifYSi"
      },
      "outputs": [],
      "source": [
        "class Translator:\n",
        "    def __init__(self, model, eng_tokenizer, kor_tokenizer, device, max_length=128):\n",
        "        self.model = model\n",
        "        self.eng_tokenizer = eng_tokenizer\n",
        "        self.kor_tokenizer = kor_tokenizer\n",
        "        self.device = device\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def translate_greedy(self, text):\n",
        "        self.model.eval()\n",
        "\n",
        "        # Tokenize English input\n",
        "        enc_input = self.eng_tokenizer(text,\n",
        "                                     return_tensors='pt',\n",
        "                                     max_length=self.max_length,\n",
        "                                     truncation=True,\n",
        "                                     padding='max_length')\n",
        "\n",
        "        # Move to device\n",
        "        enc_input = enc_input['input_ids'].to(self.device)\n",
        "\n",
        "        # Initialize decoder input with start token\n",
        "        dec_input = torch.tensor([[self.kor_tokenizer.cls_token_id]]).to(self.device)\n",
        "\n",
        "        output_tokens = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(self.max_length):\n",
        "                # Get model output\n",
        "                output = self.model(enc_input, dec_input)\n",
        "\n",
        "                # Get the next token prediction\n",
        "                next_token_logits = output[:, -1, :]\n",
        "                next_token = next_token_logits.argmax(dim=-1).unsqueeze(0)\n",
        "\n",
        "                # Break if we predict the end token\n",
        "                if next_token.item() == self.kor_tokenizer.sep_token_id:\n",
        "                    break\n",
        "\n",
        "                output_tokens.append(next_token.item())\n",
        "                # Update decoder input\n",
        "                dec_input = torch.cat([dec_input, next_token], dim=1)\n",
        "\n",
        "        # Decode the generated tokens\n",
        "        translated_text = self.kor_tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
        "        return translated_text\n",
        "\n",
        "    def translate_beam_search(self, text, beam_size=3):\n",
        "        self.model.eval()\n",
        "\n",
        "        # Tokenize English input\n",
        "        enc_input = self.eng_tokenizer(text,\n",
        "                                     return_tensors='pt',\n",
        "                                     max_length=self.max_length,\n",
        "                                     truncation=True,\n",
        "                                     padding='max_length')\n",
        "        enc_input = enc_input['input_ids'].to(self.device)\n",
        "\n",
        "        # Initialize with start token\n",
        "        start_token = torch.tensor([[self.kor_tokenizer.cls_token_id]]).to(self.device)\n",
        "        beams = [(start_token, 0.0)]\n",
        "        completed_sequences = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(self.max_length):\n",
        "                candidates = []\n",
        "\n",
        "                for dec_input, score in beams:\n",
        "                    if dec_input[0, -1].item() == self.kor_tokenizer.sep_token_id:\n",
        "                        completed_sequences.append((dec_input, score))\n",
        "                        continue\n",
        "\n",
        "                    # Get model output\n",
        "                    output = self.model(enc_input, dec_input)\n",
        "                    next_token_logits = output[:, -1, :]\n",
        "                    next_token_probs = torch.log_softmax(next_token_logits, dim=-1)\n",
        "\n",
        "                    # Get top k tokens\n",
        "                    values, indices = next_token_probs[0].topk(beam_size)\n",
        "\n",
        "                    for value, token_id in zip(values, indices):\n",
        "                        new_dec_input = torch.cat([dec_input,\n",
        "                                                 token_id.unsqueeze(0).unsqueeze(0)],\n",
        "                                                dim=1)\n",
        "                        candidates.append((new_dec_input, score + value.item()))\n",
        "\n",
        "                # Keep top beam_size candidates\n",
        "                candidates = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
        "                beams = candidates[:beam_size]\n",
        "\n",
        "                # Early stopping if we have enough completed sequences\n",
        "                if len(completed_sequences) >= beam_size:\n",
        "                    break\n",
        "\n",
        "        # Choose the sequence with highest score\n",
        "        if completed_sequences:\n",
        "            best_sequence = max(completed_sequences, key=lambda x: x[1])[0]\n",
        "        else:\n",
        "            best_sequence = max(beams, key=lambda x: x[1])[0]\n",
        "\n",
        "        # Decode tokens (exclude start token)\n",
        "        output_tokens = best_sequence[0][1:].cpu().numpy().tolist()\n",
        "        translated_text = self.kor_tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
        "\n",
        "        return translated_text\n",
        "\n",
        "# Example usage:\n",
        "def test_translation(model, test_sentence):\n",
        "    translator = Translator(model, eng_tokenizer, kor_tokenizer, device)\n",
        "    print(\"Input:\", test_sentence)\n",
        "    print(\"Greedy translation:\", translator.translate_greedy(test_sentence))\n",
        "    print(\"Beam search translation:\", translator.translate_beam_search(test_sentence, beam_size=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uFQCntgfYSi"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example usage:\n",
        "def test_translation(model_path, test_sentence):\n",
        "    # Load the saved model\n",
        "    model = Transformer(config)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Create translator instance\n",
        "    translator = Translator(model, eng_tokenizer, kor_tokenizer, device)\n",
        "\n",
        "    # Test greedy search\n",
        "    print(\"Input:\", test_sentence)\n",
        "    print(\"Greedy translation:\", translator.translate_greedy(test_sentence))\n",
        "    print(\"Beam search translation:\", translator.translate_beam_search(test_sentence, beam_size=3))\n",
        "\n",
        "# Test the model\n",
        "test_sentence = \"these gorillas were murdered\"\n",
        "test_translation(\"test_model.pth\", test_sentence)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}